---
title: "Extra Exp O-MINDS Analyses"
author: "Hyuna Cho"
date: "20/07/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

These analyses examine the differences in effect size memorability across the RANDOM and SOFTWARE conditions. 

RANDOM = images were selected at random, and counterbalanced across batches (e.g. which images were seen in the old vs new conditions)
SOFTWARE = images were selected using the O-MINDS software, to minimize variability, and counterbalanced across batches.

# Load packages and data

```{r Load packages and data}

rm(list = ls()) # clear environment

library(lme4)
library(lmerTest)
library(tidyverse)
library(data.table) # used extensively for data cleaning and analysis
library(dtplyr)
library(effsize)
library(pwr)
library(Rmisc)
library(car)
library(esquisse)
library(ggpubr)

retrieval <- read.csv("Extra_retrieval.csv") # currently in the cleaned folder

retrieval <- as.data.table(retrieval)
retrieval[, HR := mean(correct), by = participant] # hit rate by participant
retrieval[, avg_mem := correct - HR, by = participant] # participant-corrected hit rate

```

```{r Split by condition}

random = retrieval[condition == "random"]

software = retrieval[condition == "software"]

# useful for grabbing specific number of stimuli later
tmp_software <- separate(data = software,
                col = stimulus, sep = 1,
                into = c("letter", "number"))
tmp_software <- separate(data = tmp_software,
                col = number, sep = ".jpg",
                into = c("number", "jpg"))

```



# All Stimuli

Paired Cohen's D comparing effect sizes of RANDOM and SOFTWARE conditions.

When we compare the conditions (1 vs 3) in Random and Software, we find that there's a larger effect size in the random condition.
```{r Effect size; all stimuli}

# get avg correct/accuracy rates by participant
all_HR <- retrieval[,.(mean = mean(correct)), 
                               by = .(condition, participant, old_object_condition)]

# RANDOM condition 3 vs 1 exposures (paired Cohen's D)
all_cohensd_random_HR = cohen.d(
  all_HR[condition == "random" & old_object_condition == 3]$mean, 
  all_HR[condition == "random" & old_object_condition == 1]$mean, 
  paired = T) # 0.94

# SOFTWARE condition 3 vs 1 exposures (paired Cohen's D)
all_cohensd_software_HR = cohen.d(
  all_HR[condition == "software" & old_object_condition == 3]$mean, 
  all_HR[condition == "software" & old_object_condition == 1]$mean, 
  paired = T) # 0.86

```

ANOVA for interactions between # exposures and condition. # exposures is significant, but not condition or the interaction.

```{r ANOVA for interactions; all stimuli}

# compare 1 vs 3 exposures
all_ANOVA = aov(correct ~ condition * old_object_condition,
                data = retrieval[old_object_condition %in% c(1,3)])

# different ways of examining output
summary(all_ANOVA)
model.tables(all_ANOVA, type = "means", se = T)
TukeyHSD(all_ANOVA, which = "old_object_condition")

```

Power analysis
TODO: Not sure where .437 and .525 came from. Is it the estimate from earlier?

```{r Power analysis; all stimuli}

# power analysis for random (n = )
pwr.t.test(d = all_cohensd_random_HR$estimate, sig.level = .05, power = .8, type = ("paired")) 

# power analysis for software (n = )
pwr.t.test(d = all_cohensd_software_HR$estimate, sig.level = .05, power = .8, type = ("paired"))

```



# Estimate 10 per condition

What happens if we used fewer stimuli per condition? Here, we simulate what may have occurred if we presented participants with 10 images per condition.

The software should select the best-matching 10, but randomly assigned groups may differ in which stimuli are selected. As such, we bootstrapped our random analyses, finding the average effect of random selection.

```{r Function to grab 10 for the RANDOM condition}

SelectRandomOld <- function(n=10){
  
  tmp = as.data.table(matrix(nrow=0, ncol=ncol(random)))
  names(tmp) = names(random)
  
  for(b in 1:8){
    tmp1 <- random[batch == b & old_object_condition == 3]
    tmp1_stim <- as.vector(unique(tmp1$stimulus))
    tmp1_sel <- sample(tmp1_stim, n)
    # only grab if it was seen in the encoding phase?
    tmp = rbind(tmp, random[stimulus %in% tmp1_sel & old_object_condition %in% c(1,3)], fill=T)
  }
  return(tmp)
}

SelectRandomAll <- function(n=10){
  
  tmp = as.data.table(matrix(nrow=0, ncol=ncol(random)))
  names(tmp) = names(random)
  
  for(b in 1:8){
    tmp1 <- random[batch == b & old_object_condition == 3]
    tmp1_stim <- as.vector(unique(tmp1$stimulus))
    tmp1_sel <- sample(tmp1_stim, n)
    tmp = rbind(tmp, random[stimulus %in% tmp1_sel & old_object_condition %in% c(1,3,".")], fill=T)
  }
  return(tmp)
}
```

```{r Grab 10 for the SOFTWARE condition}

# tmp_software <- separate(data = software,
#                 col = stimulus, sep = 1,
#                 into = c("letter", "number"))
# tmp_software <- separate(data = tmp,
#                 col = number, sep = ".jpg",
#                 into = c("number", "jpg"))

# grab the first 10 items of each software-allocated group 
software10 <- tmp_software[number %in% c(1:10) & old_object_condition %in% c(1,3,".")]

# re-form the stimulus column
software10$stimulus = with(software10, paste0(letter, number, ".jpg"))
software10$letter = NULL
software10$number = NULL
software10$jpg = NULL

```

## Effect size using hit rate

Paired Cohen's D comparing effect sizes of RANDOM and SOFTWARE conditions, if there were 10 stimuli per group.

```{r Effect size using hit rate; randomly generated 10}

# TODO: I made this into a function later on due to later analyses; Need to change inner variables.
RandomSelectionEffectSize <- function(n_stim = 10, n_iter = 100){
  effect_table_random <- c()

  # simulate many times
  for(i in 1:n_iter){
    # get mean accuracy for n random stimuli
    tmp = SelectRandomOld(n=n_stim)[, .(mean = mean(correct)), 
                                      by = .(participant, old_object_condition)]
    
    # if they don't have a value for all 3 conditions, a paired samples effect size will not work
    goodparticipants = tmp[, .N, by=participant]
    goodparticipants = goodparticipants[N == 2,]$participant
    
    tmp = tmp[participant %in% goodparticipants]
    
    # find effect size
    ef = cohen.d(tmp[old_object_condition == 3]$mean,
                 tmp[old_object_condition == 1]$mean, paired = T)
    
    effect_table_random <- c(effect_table_random, ef$estimate)
  }
  return(effect_table_random)
}

random10_cohensd_HR = RandomSelectionEffectSize(n_stim = 10, n_iter = 100)


# look at the effect sizes
mean(random10_cohensd_HR)
hist(random10_cohensd_HR)

# calculate p-value (percentage of effect sizes above the software)
length(random10_cohensd_HR[random10_cohensd_HR > software10_cohensd$estimate])/100

```

```{r Effect size using HR; 10 stimuli per group by software}

# get avg correct/accuracy rates by participant
software10_HR <- software_10[,.(mean = mean(correct)), 
                               by = .(participant, old_object_condition)]

# SOFTWARE condition 3 vs 1 exposures (paired Cohen's D)
software10_cohensd = cohen.d(
  software10_HR[old_object_condition == 3]$mean, 
  software10_HR[old_object_condition == 1]$mean, 
  paired = T) # 0.86

```

## Effect size, using corrected hit rate.

```{r Effect size using corrected HR, randomly generated 10}

random10_cohensd_corrHR <- c()

# simulate multiple times
for(i in 1:100){
  # get mean accuracy for 10 random stimuli
  random10 = SelectRandomOld(n=10)[, .(mean = mean(avg_mem)), 
                                    by = .(participant, old_object_condition)]
  
  # if they don't have a value for all 3 conditions, a paired samples effect size will not work
  goodparticipants = random10[, .N, by=participant]
  goodparticipants = goodparticipants[N == 3,]$participant
  
  random10 = random10[participant %in% goodparticipants]
  
  # find effect size
  ef = cohen.d(random10[old_object_condition == 3]$mean,
               random10[old_object_condition == 1]$mean, 
               paired = T)
  random10_cohensd_corrHR <- c(random10_cohensd_corrHR, ef$estimate)
}

# look at the effect sizes
mean(random10_cohensd_corrHR)
hist(random10_cohensd_corrHR)

# calculate p-value (percentage of effect sizes above the software)
length(random10_cohensd_corrHR[random10_cohensd_corrHR > software10_cohensd$estimate])/100

```

```{r Effect size using corrected HR; 10 stim per group by software}

software10_corrHR <- software_10[,.(mean = mean(avg_mem)),
                               by = .(participant, old_object_condition)]

# SOFTWARE condition 3 vs 1 exposures (paired Cohen's D)
software10_cohensd_corrHR = cohen.d(
  software10_corrHR[old_object_condition == 3]$mean, 
  software10_corrHR[old_object_condition == 1]$mean, 
  paired = T)


```

## Effect size, using dprime

```{r dprime helper function}

CalculateDprime <- function(dprime_table){
  
  stats_by_participant = dcast(dprime_table,
                           value.var = "correct",
                           participant ~ old_object_condition,
                           fill=0,
                           mean, na.rm=T)
  
  # stats_by_participant$V1 = NULL
  names(stats_by_participant)[2:4] = c("FA", "HR_1", "HR_3") # rename to be more descriptive
  stats_by_participant[, FA := 1-FA] # make FA column actually correspond to FA
  stats_by_participant[FA==0, FA := 1/80][FA==1, FA := 1-(1/80)] # correct 0s and 1s
  stats_by_participant[HR_1==0, HR_1 := 1/80][HR_1==1, HR_1 := 1-(1/80)]
  stats_by_participant[HR_3==0, HR_3 := 1/80][HR_3==1, HR_3 := 1-(1/80)]
  stats_by_participant[, dprime_1 := qnorm(HR_1) - qnorm(FA)]
  stats_by_participant[, dprime_3 := qnorm(HR_3) - qnorm(FA)]
  
  return(stats_by_participant)
}

```


```{r Effect size using dprime; randomly generated 10}

random10_cohensd_dprime <- c()

for (i in 1:100) {
  
  # 10 images per group; old and new
  random10 = SelectRandomAll(n=10)
  # dprime using participants' responses
  random10_by_participant = CalculateDprime(random10)
  
  ef = cohen.d(random10_by_participant$dprime_3, 
               random10_by_participant$dprime_1, 
               paired = T)
  effect_sizes <- c(random10_cohensd_dprime, ef$estimate)
}

mean(random10_cohensd_dprime)
hist(random10_cohensd_dprime)

```

```{r Effect size using dprime; 10 stimuli per group by software}

# Calculate dprime
software10_dprime = CalculateDprime(software10)

software10_cohensd_dprime = cohen.d(software10_dprime$dprime_3, 
                                    software10_dprime$dprime_1, paired = T)

```




# Estimate 50 per condition

We can re-use the function to get 50 random stimuli, but we'll have to make a new table for the top 50 stimuli selected by the software, per condition.

```{r Grab 50 for the SOFTWARE condition}

# Run if you have not run chunk 7 (get 10 for software condition)

# tmp_software <- separate(data = software,
#                 col = stimulus, sep = 1,
#                 into = c("letter", "number"))
# tmp_software <- separate(data = tmp,
#                 col = number, sep = ".jpg",
#                 into = c("number", "jpg"))

software50 = tmp_software[number %in% c(1:50) & old_object_condition %in% c(1,3,".")]

# re-form the stimulus column
software50$stimulus = with(software50, paste0(letter, number, ".jpg"))
software50$letter = NULL
software50$number = NULL
software50$jpg = NULL

```

TODO: repeat all chunks for calculating using HR, corrHR, and dprime under estimating 10



# Plotting changes in effect size as the number of stimuli per condition increases

## Create effect size change table

```{r Initialize effect size change table}

# the stimuli-per-group you're interested in looking at
n_stim_list = seq(1,80,1)

# create a table to populate; for both random and software
plot_ef <- data.frame(matrix(ncol = 5, 
                             nrow = length(n_stim_list)*2))
colnames(plot_ef) <- c("stimpergroup","mean","group","CI_upper","CI_lower")
plot_ef$stimpergroup <- rep(n_stim_list, each=2)
plot_ef$group <- rep(c("Random","Software"))

```

```{r Simulate random values' effect size change}

index = 1
iter = 1000

# NOTE: recording each value for each nstim and iter
random_effsize <- as.data.frame(matrix(data=NA, nrow=iter, ncol = length(n_stim_list)))
names(random_effsize) <- n_stim_list

# NOTE: this takes a while (~1h)
for(i in n_stim_list){
  rand = RandomSelectionEffectSize(n_stim = i, n_iter = iter)
  
  random_effsize[,paste0(i)] = rand
  
  sum_rand = CI(rand)
  # add CI to plot_ef table
  plot_ef[index, 2] = sum_rand[2] # mean
  plot_ef[index, 4] = sum_rand[1] # upper
  plot_ef[index, 5] = sum_rand[3] # lower
  index = index + 2
}

# plot_ef = read.csv("plotef_full.csv") # pre-simulated results

```

```{r Simulate software values' effect size change}

# for the software
index = 2*3
for(i in n_stim_list[-c(1,2)]){
  
  sftwr = tmp_software[number %in% c(1:i), 
                       .(mean = mean(avg_mem)), 
                       by = .(participant, old_object_condition)] 
  
  sftwr_ef = cohen.d(sftwr[old_object_condition == 3]$mean, 
                     sftwr[old_object_condition == 1]$mean, 
                     paired = T)
  # Add to plot_ef table
  plot_ef[index, 2] = sftwr_ef$estimate
  index = index + 2
}

```

## Plot changes

```{r Plot effect sizes as number of stimuli per group increases}

ef_graph <- ggplot(plot_ef, aes(x = stimpergroup, y = mean, color = group)) +
  geom_line() +
  ylab("effect size") +
  geom_errorbar(aes(ymin=CI_lower, ymax=CI_upper), width = .7) +
  
  

```

```{r}

# Calculate average % increase in effect size across the numbers we have 
# NOTE: 1 and 2 aren't working since uneven numbers of images for 1 and 2
mean((plot_ef[plot_ef$stimpergroup > 2 & plot_ef$group == "Software", "mean"] - plot_ef[plot_ef$stimpergroup > 2 & plot_ef$group == "Random", "mean"] ) * 100) # 52.5883


# Calculate how many simulations (random) are smaller than software effect sizes
sftwr_effsize <- as.data.frame(t(plot_ef[plot_ef$stimpergroup > 2 & plot_ef$group == "Software", "mean"]))
names(sftwr_effsize) <- seq(3,80,1)

all_effsize <- rbind(sftwr_effsize, random_effsize[-c(1,2)])

random_effsize[-c(1,2)] < sftwr_effsize

t_random_effsize = t(random_effsize[-c(1,2)])
t_sftwr_effsize = t(sftwr_effsize)

t_cmpr_effsize = as.data.frame(matrix(data = NA, nrow = 2, ncol = ncol(t_random_effsize)))
names(t_cmpr_effsize) = names(t_random_effsize)

for(i in 1:nrow(t_random_effsize)){
  tmp = t_random_effsize[i,] < t_sftwr_effsize[i]
  t_cmpr_effsize = rbind(t_cmpr_effsize, tmp)
  
}

# How many times (out of simulated) is it smaller? nearly all of the time
mean(apply(t(t_cmpr_effsize[-c(1,2),]), 2, sum))

```


```{r Power analyses}

setDT(plot_ef)

t.test(plot_ef[group == "Random" &stimpergroup == 5]$mean,plot_ef[group == "Software" & stimpergroup ==5]$mean)

# HOW MANY SUBJ ARE NEEDED?

# power analysis for random (n = 13)
pwr.t.test(d = plot_ef[group == "Random" & stimpergroup == 10]$mean, sig.level = .05, power = .8, type = ("paired"))

# power analysis for software (n = 6)
pwr.t.test(d = plot_ef[group == "Software" & stimpergroup ==10]$mean, sig.level = .05, power = .8, type = ("paired"))



# HOW MUCH POWER DO WE HAVE?

pwr.t.test(
  n = 
  d = plot_ef[group == "Random" & stimpergroup == 10]$mean, sig.level = .05, power = .8, type = ("paired"))

# power analysis for software (n = 6)
pwr.t.test(d = plot_ef[group == "Software" & stimpergroup ==10]$mean, sig.level = .05, power = .8, type = ("paired"))

```

```{r publication plots for effect size}

esquisser()

Effect_Sizes <- ggplot(plot_ef, aes(x = stimpergroup, y = mean, color = group)) +
  geom_line() +
  geom_errorbar(aes(ymin=CI_lower, ymax=CI_upper), width = .7, color = "black") +
  scale_color_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  theme_classic() +
  labs(x = "Number of Stimuli Per Group", y = "Effect Size", color = "Group")
  

```


# Circular logic: verify the software is working

Is the software minimizing variability between the two conditions more than the random group?

```{r import values}

# get full data frame (includes all trials)

retrieval_full <- read.csv("clean_scripts/extra_retrieval_full.csv") # currently in the cleaned folder
retrieval_full <- setDT(retrieval_full)


### random

app_values = read.csv("full_dt_pf.csv")
setDT(app_values)

#app_values = select(app_values, stimulus, memorability, score_emotional, score_humanmade, score_outdoors, score_shoebox, h_index, unique)

random = retrieval_full[condition == "random"]


### software

app_values_soft = read.csv("experiment_stimuli/summary.csv")

app_values_soft$newname = str_split(app_values_soft$newname, "/")
app_values_soft$stimulus = sapply(app_values_soft$newname, "[[", 2)

app_values_soft = select(app_values_soft, stimulus, memorability, score_emotional, score_humanmade, score_outdoors, score_shoebox, h_index, unique)

software = retrieval_full[condition == "software"]

# useful for grabbing specific number of stimuli later
tmp_software <- separate(data = software,
                col = stimulus, sep = 1,
                into = c("letter", "number"))
tmp_software <- separate(data = tmp_software,
                col = number, sep = ".jpg",
                into = c("number", "jpg"))
tmp_software$X = NULL

```

```{r functions for random and software}

### random

rand_diff_fun <- function(variable) {
  
  variable_tbl <- data.frame(matrix(ncol = 3, nrow =0))
  colnames(variable_tbl) <- c("diff","n","i")

  for(n in n_stim_list){ 
  
    for (i in 1:1000) {
      
      rand = app_values[sample(nrow(app_values), 2*n), ] 
      rand$old_object_condition <- 1
      rand[1:n,]$old_object_condition <- 3
      setDT(rand)
      rand = rand %>% select(!!variable, old_object_condition)
      colnames(rand) <- c("metric", "old_object_condition")
      mean_df = rand[, .(mean =mean(metric)), by = .(old_object_condition)]
      difference = data.frame(matrix(ncol = 3, nrow = 1))
      colnames(difference) <- c("diff","n","i")
      difference$diff = abs(mean_df[old_object_condition == 3]$mean - mean_df[old_object_condition == 1]$mean)
      difference$n = n
      difference$i = i
      variable_tbl = rbind(variable_tbl, difference)

    }
  }
  return(variable_tbl)
}


### software

soft_diff_fun <- function(variable) {
  
  variable_tbl <- data.frame(matrix(ncol = 3, nrow =0))
  colnames(variable_tbl) <- c("diff","n","i")

  for (n in n_stim_list){ 
  
    software_selection = tmp_software[number %in% c(1:n)] 
    software_selection$stimulus = with(software_selection, paste0(letter, number, ".jpg"))
    soft = left_join(software_selection, app_values_soft)
    soft = soft %>% select(!!variable, old_object_condition)
    colnames(soft) <- c("metric", "old_object_condition")
    mean_df = soft[, .(mean =mean(metric)), by = .(old_object_condition)]
    difference = data.frame(matrix(ncol = 2, nrow = 1))
    colnames(difference) <- c("diff","n")
    difference$diff = abs(mean_df[old_object_condition == 3]$mean - mean_df[old_object_condition == 1]$mean)
    difference$n = n
    variable_tbl = rbind(variable_tbl, difference)
  }
  return(variable_tbl)
}


```


## Memorability

```{r run function - memorability}

mem_differences <- rand_diff_fun(variable = "memorability")

mem_differences_soft <- soft_diff_fun(variable = "memorability")
```

```{r plot memorability}

mem_differences$group = "Random"
setDT(mem_differences)
mem_differences_group = mem_differences[, .(diff = mean(diff), CI_upper = CI(diff)[[1]], CI_lower= CI(diff)[[3]]), by = .(n, group)]

mem_differences_soft$group = "Software"

plot_mem = rbind(mem_differences_group, mem_differences_soft, fill = TRUE)


mem_plot <- ggplot(plot_mem, aes(x = n, y = diff, color = group)) +
  geom_line() +
  geom_errorbar(aes(ymin=CI_lower, ymax=CI_upper), width = .7, color = "black") +
  scale_color_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  theme_classic() +
  labs(x = "", y = "Mean \nMemorability \nDifference", color = "Assignment") +
  theme(axis.title.y = element_text(angle=0))

###statistics

mem_join <- left_join(mem_differences, mem_differences_soft, by = "n")

mem_join[, compare := 0]
mem_join[diff.x < diff.y , compare := 1]
mem_join[, .(mean = mean(compare))]
mem_join[, .(mean = mean(compare)), by = .(n)] #what percentage of simulated runs is the random value below the software value


```


## Emotionality

```{r run function - emotion}

emo_differences <- rand_diff_fun(variable = "score_emotional")

emo_differences_soft <- soft_diff_fun(variable = "score_emotional")
```

```{r plot emotionality}

emo_differences$group = "Random"
setDT(emo_differences)
emo_differences_group = emo_differences[, .(diff = mean(diff), CI_upper = CI(diff)[[1]], CI_lower= CI(diff)[[3]]), by = .(n, group)]

emo_differences_soft$group = "Software"

plot_emo = rbind(emo_differences_group, emo_differences_soft, fill = TRUE)

emo_plot <- ggplot(plot_emo, aes(x = n, y = diff, color = group)) +
  geom_line() +
  geom_errorbar(aes(ymin=CI_lower, ymax=CI_upper), width = .7, color = "black") +
  scale_color_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  theme_classic() +
  labs(x = "", y = "Mean \nEmotionality \nDifference", color = "Assignment") +
  theme(axis.title.y = element_text(angle=0))

###statistics

emo_join <- left_join(emo_differences, emo_differences_soft, by = "n")

emo_join[, compare := 0]
emo_join[diff.x < diff.y , compare := 1]
emo_join[, .(mean = mean(compare))]
emo_join[, .(mean = mean(compare)), by = .(n)] #what percentage of simulated runs is the random below the software

```

## Nameability


```{r find agreement level}

####h-index is a measure of name agreement. 

##Plot the h-index for condition 1 and condition 2 of the software

soft = left_join(software[old_object_condition %in% c(1,3)], app_values_soft, by = 'stimulus')

soft = soft[!duplicated(soft$stimulus),] #only need 1 row of the stimulus

ggplot(soft, aes(x = h_index, fill = old_object_condition)) +
  geom_density(alpha=.3)

# do this with only 20 stimuli per condition

software20 <- tmp_software[number %in% c(1:20) & old_object_condition %in% c(1,3,".") ]
software20 = software20[letter %in% c("A","B")]

# re-form the stimulus column
software20$stimulus = with(software20, paste0(letter, number, ".jpg"))
software20 = left_join(software20, app_values_soft, by = "stimulus")
software20 = software20[!duplicated(software20$stimulus),]

ggplot(software20, aes(x = h_index, fill = old_object_condition)) +
  geom_density(alpha=.3)

software20$group <- "software"

##Plot the h-index for a random foil

rand = app_values[sample(nrow(app_values), 40), ] 
rand$old_object_condition <- 1
rand[1:20,]$old_object_condition <- 3

#what's the mean difference between conditions? is it representative? If it was, it would be about 0.25

mean_name = rand[, .(mean =mean(h_index)), by = .(old_object_condition)]
abs(mean_name[old_object_condition == 3]$mean - mean_name[old_object_condition == 1]$mean)

#save one that is close to 0.25
rep_rand_name <- rand

rep_rand_name$old_object_condition <- as.factor(rep_rand_name$old_object_condition)

ggplot(rep_rand_name, aes(x = h_index, fill = old_object_condition)) +
  geom_density(alpha=.3)

rep_rand_name$group <- "Random"


combined_name_20 <- rbind(rep_rand_name, software20, fill = TRUE)

ggplot(combined_name_20, aes(x = h_index, fill = old_object_condition)) +
  geom_density(alpha=.3) +
  facet_grid(~group)



#plot h-index density for all stimuli
ggplot(app_values, aes(x = h_index)) +
  geom_density(alpha=.3)

#overlay the two plots

soft[, group := "software"]

combined_app_values <- rbind(soft, app_values, fill = TRUE)

combined_app_values[is.na(group) == TRUE, group := "all"]

ggplot(combined_app_values, aes(x = h_index, fill = group)) +
  geom_density(alpha = .5)



```


```{r run function - nameability}

name_differences <- rand_diff_fun(variable = "h_index")

name_differences_soft <- soft_diff_fun(variable = "h_index")
```

```{r plot nameability}

name_differences$group = "Random"
setDT(name_differences)
name_differences_group = name_differences[, .(diff = mean(diff), CI_upper = CI(diff)[[1]], CI_lower= CI(diff)[[3]]), by = .(n, group)]

name_differences_soft$group = "Software"

plot_name = rbind(name_differences_group, name_differences_soft, fill = TRUE)

name_plot <- ggplot(plot_name, aes(x = n, y = diff, color = group)) +
  geom_line() +
  geom_errorbar(aes(ymin=CI_lower, ymax=CI_upper), width = .7, color = "black") +
  scale_color_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  theme_classic() +
  labs(x = "Number of Stimuli Per Group", y = "Mean \nNameability \nDifference", color = "Assignment") +
  theme(axis.title.y = element_text(angle=0))

###statistics

name_join <- left_join(name_differences, name_differences_soft, by = "n")

name_join[, compare := 0]
name_join[diff.x < diff.y , compare := 1]
name_join[, .(mean = mean(compare))]
name_join[, .(mean = mean(compare)), by = .(n)] #what percentage of simulated runs is the random below the software

```

```{r arrange plots for publication}

Condition_Differences <- ggarrange(mem_plot, emo_plot, name_plot, nrow = 3, ncol = 1, common.legend = TRUE)

```

## Shoebox

We need to run the software again, because we didn't select for any of the orientation questions in the extra experiment. Here we ran the software with the only criteria selected being shoebox (no memorability, emotionality, etc.)

```{r find agreement level - shoebox}

shoebox_summary <- read.csv("shoebox20_summary.csv")
setDT(shoebox_summary)
#score shoebox is the average score of if it's larger or smaller than a shoebox. 1 = smaller than a shoebox, 0 = larger than a shoebox

shoebox_summary[, agreement := abs(score_shoebox - .5) +.5]

ggplot(shoebox_summary, aes(x = agreement)) +
  geom_histogram() +
  scale_x_continuous(limits = c(0,1.2))


#plot shoebox score density for all stimuli
ggplot(app_values, aes(x = score_shoebox)) +
  geom_histogram()

#randomly select 160 stimuli and plot shoebox

shoe_rand <- app_values[sample(nrow(app_values), 40), ]
shoe_rand[, agreement := abs(score_shoebox - .5) +.5]

ggplot(shoe_rand, aes(x = score_shoebox)) +
  geom_histogram() +
  scale_x_continuous(limits = c(0,1.2))

#overlay the two plots

shoebox_summary[, group := "software"]
shoe_rand[, group := "random"]

combined_shoe <- rbind(shoebox_summary, shoe_rand, fill = TRUE)

ggplot(combined_shoe, aes(x = agreement, fill = group)) +
  geom_density(alpha = .4) +
  scale_x_continuous(name = "Shoebox Score Agreement")

```


## Human-made


```{r find agreement level - humanmade}

human_summary <- read.csv("humanmade20_summary.csv")
setDT(human_summary)
#score humanmade is the average score of if it's natural or humanmade. 1 = humanmade, 0 = natural

human_summary[, agreement := abs(score_humanmade - .5) +.5]

ggplot(human_summary, aes(x = agreement)) +
  geom_histogram() +
  scale_x_continuous(limits = c(0,1.2))


#plot humanmade score density for all stimuli
ggplot(app_values, aes(x = score_humanmade)) +
  geom_histogram()

#randomly select 20 stimuli per condition and plot humanmade scores

human_rand <- app_values[sample(nrow(app_values), 40), ]
human_rand[, agreement := abs(score_humanmade - .5) +.5]

ggplot(human_rand, aes(x = score_humanmade)) +
  geom_histogram() +
  scale_x_continuous(limits = c(0,1.2))

#overlay the two plots

human_summary[, group := "software"]
human_rand[, group := "random"]

combined_human <- rbind(human_summary, human_rand, fill = TRUE)

ggplot(combined_human, aes(x = agreement, fill = group)) +
  geom_density(alpha = .4) +
  scale_x_continuous(limits = c(0.49,1.01), name = "Humanmade Score Agreement")

```



## Outdoors

```{r find agreement level - outdoors}

outdoors_summary <- read.csv("outdoors20_summary.csv")
setDT(outdoors_summary)
#score outdoors is the average score of if it's normally found indoors or outdoors. 1 = outdoors, 0 = indoors

outdoors_summary[, agreement := abs(score_outdoors - .5) +.5]

ggplot(outdoors_summary, aes(x = agreement)) +
  geom_histogram() +
  scale_x_continuous(limits = c(0,1.2))


#plot outdoors score density for all stimuli
ggplot(app_values, aes(x = score_outdoors)) +
  geom_histogram()

#randomly select 160 stimuli and plot shoebox

outdoors_rand <- app_values[sample(nrow(app_values), 40), ]
outdoors_rand[, agreement := abs(score_outdoors - .5) +.5]

ggplot(outdoors_rand, aes(x = score_outdoors)) +
  geom_histogram() +
  scale_x_continuous(limits = c(0,1.2))

#overlay the two plots

outdoors_summary[, group := "software"]
outdoors_rand[, group := "random"]

combined_outdoors <- rbind(outdoors_summary, outdoors_rand, fill = TRUE)

ggplot(combined_outdoors, aes(x = agreement, fill = group)) +
  geom_density(alpha = .4) +
  scale_x_continuous(limits = c(0.49,1.01), name = "Outdoors Score Agreement")

```

## Plots
```{r make publication plots}

esquisser()

shoe_plot <- combined_shoe %>%
 filter(humanmade_response %in% c("human-made", "natural") | is.na(humanmade_response)) %>%
 filter(outdoors_response %in% c("outdoors", "indoors") | is.na(outdoors_response)) %>%
 ggplot() +
  aes(x = agreement, fill = group, colour = group) +
  geom_density(adjust = 0.4, alpha = .4) +
  scale_fill_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  scale_color_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  labs(x = "Shoebox Score Agreement", y = "Frequency", fill = "Assignment", color = "Assignment") +
  theme_classic() 

human_plot <- combined_human %>%
ggplot() +
  aes(x = agreement, fill = group, colour = group) +
  geom_density(adjust = 0.4, alpha = .4) +
  scale_fill_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  scale_color_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  labs(x = "Human-made Score Agreement", y = "", fill = "Assignment", color = "Assignment") +
  theme_classic() 

outdoors_plot <- combined_outdoors %>%
ggplot() +
  aes(x = agreement, fill = group, colour = group) +
  geom_density(adjust = 0.4, alpha = .4) +
  scale_fill_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  scale_color_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  labs(x = "Indoor/Outdoor Score Agreement", y = "Frequency", fill = "Assignment", color = "Assignment") +
  theme_classic()

name_plot <- combined_name_20 %>%
  ggplot() +
  aes(x = h_index, fill = group, colour = group) +
  geom_density(adjust = 0.4, alpha = .4) +
  scale_fill_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  scale_color_manual(
    values = c("#F6B254","#674AEE"), labels = c("Random", "Software")
  ) +
  labs(x = "H-Index", y = "", fill = "Assignment", color = "Assignment") +
  theme_classic()

agreement <- ggarrange(shoe_plot,human_plot, outdoors_plot, name_plot, nrow = 2, ncol = 2, common.legend = TRUE, labels = c("A","B","C", "D"))

```

